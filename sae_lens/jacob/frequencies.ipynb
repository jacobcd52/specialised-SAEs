{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot OWT freq vs rank\n",
    "- Plot freq hists for:\n",
    "    - GSAE-phys\n",
    "    - SSAE-phys-widegsae\n",
    "    - SSAE-phys-narrowgsae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/root/specialised-SAEs\")\n",
    "from datasets import load_dataset\n",
    "from transformer_lens import utils, HookedTransformer\n",
    "import gc\n",
    "import torch\n",
    "from sae_lens.jacob.load_sae_from_hf import load_sae_from_hf\n",
    "from config import DTYPE_MAP\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "DTYPE = \"float32\"\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=\"cuda\", dtype=DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsae_64 = load_sae_from_hf(\"jacobcd52/gpt2-gsae\", \n",
    "                        \"expansion=64.safetensors\", \n",
    "                        \"expansion=64_cfg.json\",\n",
    "                        device=\"cuda\",\n",
    "                        dtype=DTYPE)\n",
    "\n",
    "gsae_16 = load_sae_from_hf(\"jacobcd52/gpt2-gsae\", \n",
    "                        \"expansion=16.safetensors\", \n",
    "                        \"expansion=16_cfg.json\",\n",
    "                        device=\"cuda\",\n",
    "                        dtype=DTYPE)\n",
    "\n",
    "ssae_64_list = [\n",
    "    load_sae_from_hf(\"jacobcd52/gpt2-ssae-phys-widegsae\",\n",
    "                    f\"l1_coeff={l1_coeff}_expansion=2_control=0.0.safetensors\",\n",
    "                    f\"l1_coeff={l1_coeff}_expansion=2_control=0.0_cfg.json\",\n",
    "                    device=\"cuda\",\n",
    "                    dtype=DTYPE)\n",
    "    for l1_coeff in [2, 3, 4, 5, 6]\n",
    "]\n",
    "\n",
    "ssae_64_list = [\n",
    "    load_sae_from_hf(\"jacobcd52/gpt2-ssae-phys-narrowgsae\",\n",
    "                    f\"l1_coeff={l1_coeff}_expansion=2_control=0.0.safetensors\",\n",
    "                    f\"l1_coeff={l1_coeff}_expansion=2_control=0.0_cfg.json\",\n",
    "                    device=\"cuda\",\n",
    "                    dtype=DTYPE)\n",
    "    for l1_coeff in [2, 3, 4, 5, 6]\n",
    "]\n",
    "\n",
    "ssae_0_list = [\n",
    "    load_sae_from_hf(\"jacobcd52/gpt2-gsae-phys\",\n",
    "                    f\"l1_coeff={l1_coeff}_expansion=2.safetensors\",\n",
    "                    f\"l1_coeff={l1_coeff}_expansion=2_cfg.json\",\n",
    "                    device=\"cuda\",\n",
    "                    dtype=DTYPE)\n",
    "    for l1_coeff in [2, 3, 4, 5, 6]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get OWT tokens\n",
    "data = load_dataset(\"stas/openwebtext-10k\", split=\"train\")\n",
    "tokenized_data = utils.tokenize_and_concatenate(data, model.tokenizer, max_length=256)\n",
    "tokenized_data = tokenized_data.shuffle(42)\n",
    "owt_tokens = tokenized_data[\"tokens\"][:20_000].cuda()\n",
    "print(\"owt_tokens has shape\", owt_tokens.shape)\n",
    "print(\"total number of tokens:\", int(owt_tokens.numel()//1e6), \"million\")\n",
    "print()\n",
    "\n",
    "# get physics-papers tokens\n",
    "data = load_dataset(\"jacobcd52/physics-papers\", split=\"train[:10%]\")\n",
    "# Define a filter function to remove null entries\n",
    "def remove_null_entries(example):\n",
    "    return all(value is not None and value != '' for value in example.values())\n",
    "# Apply the filter to remove null entries\n",
    "data = data.filter(remove_null_entries)\n",
    "tokenized_data = utils.tokenize_and_concatenate(data, model.tokenizer, max_length=256)\n",
    "tokenized_data = tokenized_data.shuffle(42)\n",
    "phys_tokens = tokenized_data[\"tokens\"][:20_000].cuda()\n",
    "print(\"phys_tokens has shape\", phys_tokens.shape)\n",
    "print(\"total number of tokens:\", int(phys_tokens.numel()//1e6), \"million\")\n",
    "\n",
    "# clean up\n",
    "del tokenized_data, data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freqs()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
