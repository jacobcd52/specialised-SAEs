{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"/root/specialised-SAEs/\")\n",
    "import os\n",
    "from sae_lens.config import LanguageModelSAERunnerConfig\n",
    "from sae_lens.sae_training_runner import SAETrainingRunner\n",
    "from sae_lens.training.activations_store import ActivationsStore\n",
    "from sae_lens.sae import SAE\n",
    "import logging\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "from huggingface_hub import login, HfApi, create_repo\n",
    "login(token=\"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\")\n",
    "api = HfApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49cffb05af1c4c97bc05a9798ff3ce38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gemma-2-2b-it\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def filter_and_reupload(subject : str,\n",
    "                        n_ctx : int = 512,\n",
    "                        batch_size : int = 2,\n",
    "                        threshold : float = 4.0,\n",
    "                        ):\n",
    "    \n",
    "    # Load the data\n",
    "    data = load_dataset(f\"jacobcd52/{subject}\")\n",
    "    tokenized_data = utils.tokenize_and_concatenate(data[\"train\"], model.tokenizer, max_length=n_ctx)\n",
    "    tokens = tokenized_data[\"tokens\"].cuda()\n",
    "    del tokenized_data\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    n_batches = tokens.shape[0] // batch_size\n",
    "\n",
    "    # Calculate perplexity and filter data\n",
    "    filtered_data = []\n",
    "\n",
    "    for b in tqdm(range(10)):\n",
    "        batch = tokens[b*batch_size:(b+1)*batch_size].cuda()\n",
    "        loss = model(batch, return_type=\"loss\", loss_per_token=True).mean(dim=1)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            if loss[i] <= threshold:\n",
    "                filtered_data.append({\n",
    "                    \"text\": model.tokenizer.decode(batch[i].tolist(), skip_special_tokens=True),\n",
    "                })\n",
    "\n",
    "            else:\n",
    "                print(f\"Batch {b} element {i} Loss {loss[i].item():.2f}. Filtered out: \", model.tokenizer.decode(batch[i].tolist(), skip_special_tokens=True))\n",
    "    # Create a new dataset with the filtered data\n",
    "    cleaned_dataset = Dataset.from_dict({\n",
    "        \"text\": [item[\"text\"] for item in filtered_data],\n",
    "    })\n",
    "\n",
    "    # Set up Hugging Face repository\n",
    "    repo_id = f\"jacobcd52/{subject}_cleaned\"\n",
    "\n",
    "    # Check if the repository exists\n",
    "    api = HfApi()\n",
    "    try:\n",
    "        api.repo_info(repo_id, repo_type=\"dataset\")\n",
    "        print(f\"Repository {repo_id} already exists.\")\n",
    "    except Exception:\n",
    "        print(f\"Repository {repo_id} does not exist. Creating new repository...\")\n",
    "        create_repo(repo_id, repo_type=\"dataset\", private=False)\n",
    "\n",
    "    # Save the cleaned dataset locally as a JSON file\n",
    "    local_file = \"cleaned_dataset.json\"\n",
    "    with open(local_file, \"w\") as f:\n",
    "        json.dump(cleaned_dataset.to_dict(), f)\n",
    "\n",
    "    # Upload the file to the Hugging Face repository\n",
    "    try:\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=local_file,\n",
    "            path_in_repo=\"cleaned_dataset.json\",\n",
    "            repo_id=repo_id,\n",
    "            repo_type=\"dataset\"\n",
    "        )\n",
    "        print(f\"Dataset successfully uploaded to {repo_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while uploading the dataset: {str(e)}\")\n",
    "        print(\"Please check your permissions and connection, then try again.\")\n",
    "\n",
    "    # Clean up the local file\n",
    "    # os.remove(local_file)\n",
    "    \n",
    "    return cleaned_dataset, filtered_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:00<00:00, 20.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4 element 1 Loss 4.04. Filtered out:   does a patient need to become a drip of glucose. ii) Till when does a patient need to be given a glucose. iii) How does the glucose help the patient to recover. 26. If there were no green plants, all life on the earth would come to an end! Comment?(AS5) 27. Draw a neatly labeled diagram of chloroplast found in leaf, and it’s role in photosysthesis?(AS5) 28. Draw the label diagram of human digestive system? List out the parts where peristalasis takes place. (AS5) 29. Raheem prepared a model showing the passage of the food through different parts of the elementary canal? Observe this and label it’s parts. (AS5) 30. Observe the following diagram and write a note on light dependent, light independent reactions.(AS5) light Calvin Cycle Chloroplast photo chemical reaction thermo chemical reaction Photochemical reaction Thermochemical reaction 31. Almost all the living world depends on plants for food material. How do you appreciate the process of making food by the green plants?(AS6) 32. Even a hard solid food also becomes smooth slurry in the digestive system by the enzymes released at a perticular time. This mechanism is an amazing fact. Prepare a cartoon on it. (AS6) 33. What food habbits you are going to follow after reading this chapter? Why? (AS7) 22 X Class Nutrition - Food supplying system Fill in the blanks 1. The food synthesized by the plant is stored as ______________________. 2. ________________________ are be sites of photosynthesis. 3. Pancreatic juice contains enzymes for carrying the process of digestion of ___________________ and ________________________. 4. The finger like projections which increases the surface area in small intestine are called____________________. 5. The gastric juice contains _________________________ acid. 6. ___________________ vitamin sysnthesised by bacteria present in intestine. Choose the correct answer 7. Which of the following organisms take the food by parasitic nutrition? a) Yeast b) Mushrooms c) Cuscuta d) Leeches 8. The rate of Photosyntesis is not affected by: ( ( ) ) a) Light Intensity b) Humidity C) Temperature d) Carbon dioxide concentration 9. A plant is kept in dark cupboard for about forty eight hours before conducting any experiment on Photosynthesis in order to : ( a) Remove chorophy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 20.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository jacobcd52/hs_bio_cleaned already exists.\n",
      "Dataset successfully uploaded to jacobcd52/hs_bio_cleaned\n"
     ]
    }
   ],
   "source": [
    "subject_list = [\n",
    "    \"hs_bio\",\n",
    "    # \"hs_math\",\n",
    "    # \"hs_phys\",\n",
    "    # \"college_bio\",\n",
    "    # \"college_math\",\n",
    "    # \"college_phys\",\n",
    "    # \"history\",\n",
    "    # \"econ\"\n",
    "]\n",
    "\n",
    "for subject in subject_list:\n",
    "    cleaned_dataset, filtere_data = filter_and_reupload(subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blah\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1067a2b3b4bd4541906ed652c7713fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9498de37d1734216bc64cc9b6a70c06d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset updated successfully!\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from huggingface_hub import HfApi, login\n",
    "\n",
    "for subject in [\"hs_bio_cleaned\"]:\n",
    "    # Load the dataset\n",
    "    data = load_dataset(f\"jacobcd52/{subject}\", split=\"train\")\n",
    "\n",
    "    # Fix the issue: flatten the nested list\n",
    "    flattened_text = data[\"text\"][0]\n",
    "    print(\"blah\")\n",
    "    # Create a new dataset with the flattened text\n",
    "    new_data = Dataset.from_dict({\"text\": flattened_text})\n",
    "\n",
    "    # Push to the Hugging Face Hub\n",
    "    new_data.push_to_hub(f\"jacobcd52/{subject}\", split=\"train\")\n",
    "\n",
    "    print(\"Dataset updated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(f\"jacobcd52/hs_bio\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1296\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7836257fedb34f9ba19c350a7d84823c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/6.30M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1331a4ae28bc486c8c14451d77bbb125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data2 = load_dataset(f\"jacobcd52/hs_bio_cleaned\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 5350\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
