{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports & definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/jacobcd52/sae_vis.git --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f4db1e7b490>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/root/specialised-SAEs/\")\n",
    "from huggingface_hub import hf_hub_download\n",
    "from sae_lens.sae import SAE\n",
    "from sae_lens.training.training_sae import TrainingSAE\n",
    "import sae_vis\n",
    "from sae_lens.training.activations_store import ActivationsStore\n",
    "from sae_lens.config import LanguageModelSAERunnerConfig\n",
    "from sae_lens.sae_training_runner import SAETrainingRunner\n",
    "from sae_lens.jacob.load_sae_from_hf import load_sae_from_hf\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callum imports \n",
    "from IPython import get_ipython # type: ignore\n",
    "ipython = get_ipython(); assert ipython is not None\n",
    "\n",
    "# Standard imports\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import webbrowser\n",
    "import os\n",
    "from transformer_lens import utils, HookedTransformer\n",
    "from datasets.arrow_dataset import Dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "import time\n",
    "\n",
    "# Library imports\n",
    "from sae_vis.utils_fns import get_device\n",
    "from sae_vis.model_fns import AutoEncoder\n",
    "from sae_vis.data_storing_fns import SaeVisData\n",
    "from sae_vis.data_config_classes import SaeVisConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(\n",
    "    activation_store: ActivationsStore,\n",
    "    n_batches_to_sample_from: int = 2**10,\n",
    "    n_prompts_to_select: int = 4096 * 6,\n",
    "    control_mixture: float = 0.5\n",
    "):\n",
    "    all_tokens_list = []\n",
    "\n",
    "    print(\"getting control tokens\")\n",
    "    pbar = tqdm(range(int(control_mixture*n_batches_to_sample_from)))\n",
    "    for _ in pbar:\n",
    "        batch_tokens = activation_store.get_control_batch_tokens()\n",
    "        batch_tokens = batch_tokens[torch.randperm(batch_tokens.shape[0])][\n",
    "            : batch_tokens.shape[0]\n",
    "        ]\n",
    "        all_tokens_list.append(batch_tokens)\n",
    "\n",
    "    print(\"getting specialised tokens\")\n",
    "    pbar = tqdm(range(int((1-control_mixture)*n_batches_to_sample_from)))\n",
    "    for _ in pbar:\n",
    "        batch_tokens = activation_store.get_batch_tokens()\n",
    "        batch_tokens = batch_tokens[torch.randperm(batch_tokens.shape[0])][\n",
    "            : batch_tokens.shape[0]\n",
    "        ]\n",
    "        all_tokens_list.append(batch_tokens)\n",
    "\n",
    "    all_tokens = torch.cat(all_tokens_list, dim=0)\n",
    "    all_tokens = all_tokens[torch.randperm(all_tokens.shape[0])]\n",
    "    return all_tokens[:n_prompts_to_select]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 2048-L1-1-LR-0.001-Tokens-4.096e+07\n",
      "n_tokens_per_buffer (millions): 0.131072\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 10000\n",
      "Total wandb updates: 333\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 524.288\n",
      "We will reset the sparsity calculation 10 times.\n",
      "Number tokens in sparsity calculation window: 4.10e+06\n",
      "instantiating ssae\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9107543fc4714e0ba49f83d4b3d41c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2b-it into HookedTransformer\n",
      "\n",
      "model loaded\n",
      "model.cfg.dtype =  torch.float32\n",
      "model.W_E.dtype =  torch.float32\n",
      "\n",
      "using control dataset\n",
      "Downloading weights from Hugging Face Hub\n",
      "GSAE weights file saved as temp_sae/sae_weights.safetensors\n",
      "Downloading cfg from Hugging Face Hub\n",
      "GSAE cfg file saved as temp_sae/cfg.json\n",
      "Loading weights into GSAE from temp_sae/sae_weights.safetensors\n",
      "temp_sae/cfg.json temp_sae/sae_weights.safetensors\n",
      "\n",
      "gsae loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_training_steps = 10_000 \n",
    "batch_size = 4096\n",
    "total_training_tokens = total_training_steps * batch_size\n",
    "\n",
    "lr_warm_up_steps = 0\n",
    "lr_decay_steps = total_training_steps // 5  # 20% of training\n",
    "l1_warm_up_steps = total_training_steps // 20  # 5% of training\n",
    "\n",
    "control_mixture = 0.9\n",
    "lr = 1e-3\n",
    "l1_coefficient = 1\n",
    "expansion_factor = 1\n",
    "\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    # JACOB\n",
    "    gsae_repo = \"jacobcd52/gemma2-gsae\",\n",
    "    gsae_filename = \"sae_weights.safetensors\",\n",
    "    gsae_cfg_filename = \"cfg.json\",\n",
    "    is_control_dataset_tokenized=False,\n",
    "    control_mixture=control_mixture,\n",
    "    control_dataset_path=\"stas/openwebtext-10k\" if control_mixture > 0 else None,\n",
    "    context_size=128,  # will control the lenght of the prompts we feed to the model. Larger is better but slower. so for the tutorial we'll use a short one.\n",
    "    dataset_path=\"jacobcd52/physics-papers\",\n",
    "    is_dataset_tokenized=False,\n",
    "\n",
    "    # Data Generating Function (Model + Training Distribution)\n",
    "    architecture=\"gated\",  # we'll use the gated variant.\n",
    "    model_name=\"gemma-2b-it\",  # our model (more options here: https://neelnanda-io.github.io/TransformerLens/generated/model_properties_table.html)\n",
    "    hook_name=\"blocks.12.hook_resid_pre\",  # A valid hook point (see more details here: https://neelnanda-io.github.io/TransformerLens/generated/demos/Main_Demo.html#Hook-Points)\n",
    "    hook_layer=12,  # Only one layer in the model.\n",
    "    d_in=2048,  # the width of the mlp output.\n",
    "    streaming=True,  # we could pre-download the token dataset if it was small.\n",
    "    # SAE Parameters\n",
    "    mse_loss_normalization=None,  # We won't normalize the mse loss,\n",
    "    expansion_factor=expansion_factor,  # the width of the SAE. Larger will result in better stats but slower training.\n",
    "    b_dec_init_method=\"zeros\",  # The geometric median can be used to initialize the decoder weights.\n",
    "    apply_b_dec_to_input=True,  # We won't apply the decoder weights to the input.\n",
    "    normalize_sae_decoder=False,\n",
    "    scale_sparsity_penalty_by_decoder_norm=False,\n",
    "    decoder_heuristic_init=True,\n",
    "    init_encoder_as_decoder_transpose=True,\n",
    "    # normalize_activations=False, JACOB\n",
    "    # Training Parameters\n",
    "    lr=lr,  # lower the better, we'll go fairly high to speed up the tutorial.\n",
    "    adam_beta1=0.9,  # adam params (default, but once upon a time we experimented with these.)\n",
    "    adam_beta2=0.999,\n",
    "    lr_scheduler_name=\"constant\",  # constant learning rate with warmup. Could be better schedules out there.\n",
    "    lr_warm_up_steps=lr_warm_up_steps,  # this can help avoid too many dead features initially.\n",
    "    lr_decay_steps=lr_decay_steps,  # this will help us avoid overfitting.\n",
    "    l1_coefficient=l1_coefficient,  # will control how sparse the feature activations are\n",
    "    l1_warm_up_steps=l1_warm_up_steps,  # this can help avoid too many dead features initially.\n",
    "    lp_norm=1.0,  # the L1 penalty (and not a Lp for p < 1)\n",
    "    train_batch_size_tokens=batch_size,\n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer=64,  # controls how many activations we store / shuffle.\n",
    "    training_tokens=total_training_tokens,  # 100 million tokens is quite a few, but we want to see good stats. Get a coffee, come back.\n",
    "    store_batch_size_prompts=16,\n",
    "    # Resampling protocol\n",
    "    use_ghost_grads=False,  # we don't use ghost grads anymore.\n",
    "    feature_sampling_window=1000,  # this controls our reporting of feature sparsity stats\n",
    "    dead_feature_window=1000,  # would effect resampling or ghost grads if we were using it.\n",
    "    dead_feature_threshold=1e-4,  # would effect resampling or ghost grads if we were using it.\n",
    "    # WANDB\n",
    "    log_to_wandb=True,  # always use wandb unless you are just testing code.\n",
    "    wandb_project=\"scratch-ssae-stuff\",\n",
    "    run_name = f\"l1={l1_coefficient}_expansion={expansion_factor}_control_mix={control_mixture}_tokens={batch_size*total_training_steps}_lr={lr}\",\n",
    "    wandb_log_frequency=30,\n",
    "    eval_every_n_wandb_logs=20,\n",
    "    # Misc\n",
    "    device=\"cuda\",\n",
    "    seed=42,\n",
    "    n_checkpoints=1,\n",
    "    checkpoint_path=f\"phys_gpt2_ssae_l1_coeff={l1_coefficient}_expansion={expansion_factor}_control_mixture={control_mixture}_tokens={batch_size*total_training_steps}_lr={lr}\",\n",
    "    dtype=\"float32\"\n",
    ")\n",
    "print(\"instantiating ssae\")\n",
    "ssae = SAETrainingRunner(cfg)\n",
    "model = ssae.model\n",
    "activation_store = ssae.activations_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting control tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16384 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3351/16384 [00:38<02:29, 87.27it/s] \n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m all_tokens_gpt \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivation_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcontrol_mixture\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mn_batches_to_sample_from\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mn_prompts_to_select\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m, in \u001b[0;36mget_tokens\u001b[0;34m(activation_store, n_batches_to_sample_from, n_prompts_to_select, control_mixture)\u001b[0m\n\u001b[1;32m     10\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mint\u001b[39m(control_mixture\u001b[38;5;241m*\u001b[39mn_batches_to_sample_from)))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m---> 12\u001b[0m     batch_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mactivation_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_control_batch_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     batch_tokens \u001b[38;5;241m=\u001b[39m batch_tokens[torch\u001b[38;5;241m.\u001b[39mrandperm(batch_tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])][\n\u001b[1;32m     14\u001b[0m         : batch_tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     15\u001b[0m     ]\n\u001b[1;32m     16\u001b[0m     all_tokens_list\u001b[38;5;241m.\u001b[39mappend(batch_tokens)\n",
      "File \u001b[0;32m~/specialised-SAEs/sae_lens/training/activations_store.py:400\u001b[0m, in \u001b[0;36mActivationsStore.get_control_batch_tokens\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    397\u001b[0m current_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m batch_tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m batch_size:\n\u001b[0;32m--> 400\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_next_control_dataset_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m     token_len \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;66;03m# TODO: Fix this so that we are limiting how many tokens we get from the same context.\u001b[39;00m\n",
      "File \u001b[0;32m~/specialised-SAEs/sae_lens/training/activations_store.py:795\u001b[0m, in \u001b[0;36mActivationsStore._get_next_control_dataset_tokens\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    793\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_control_dataset_tokenized:\n\u001b[0;32m--> 795\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterable_control_dataset\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol_tokens_column]\n\u001b[1;32m    796\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto_tokens(\n\u001b[1;32m    798\u001b[0m             s,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    805\u001b[0m     )\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;28mlen\u001b[39m(tokens\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    808\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens.shape should be 1D but was \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokens\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_tokens_gpt = get_tokens(activation_store, \n",
    "                            control_mixture=1.0, \n",
    "                            n_batches_to_sample_from = 4096*4,\n",
    "                            n_prompts_to_select = 4096*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading weights from Hugging Face Hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58ae3ba371c4ea98303c3befc497ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)=20_tokens=40960000_lr=0.001.safetensors:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSAE weights file saved as temp_sae/sae_weights.safetensors\n",
      "Downloading cfg from Hugging Face Hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e0af579d5d43ed873a77ffe12a8b2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)eff=20_tokens=40960000_lr=0.001_cfg.json:   0%|          | 0.00/2.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSAE cfg file saved as temp_sae/cfg.json\n",
      "Loading weights into GSAE from temp_sae/sae_weights.safetensors\n",
      "temp_sae/cfg.json temp_sae/sae_weights.safetensors\n"
     ]
    }
   ],
   "source": [
    "sae = load_sae_from_hf(\"jacobcd52/gemma2-ssae-phys\", \n",
    "                       \"l1_coeff=20_tokens=40960000_lr=0.001.safetensors\", \n",
    "                       \"l1_coeff=20_tokens=40960000_lr=0.001_cfg.json\",\n",
    "                       device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get dashboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2d134bb7cd45f2a2ec26641161644a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Forward passes to cache data for vis:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0851aa9ff6924eeab74edac90c9a3b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting vis data from cached data:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Task                                           </span>┃<span style=\"font-weight: bold\"> Time    </span>┃<span style=\"font-weight: bold\"> Pct % </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━┩\n",
       "│ (1) Initialization                             │ 0.00s   │ 0.0%  │\n",
       "│ (2) Forward passes to gather model activations │ 118.18s │ 70.5% │\n",
       "│ (3) Computing feature acts from model acts     │ 35.48s  │ 21.2% │\n",
       "│ (4) Getting data for tables                    │ 0.01s   │ 0.0%  │\n",
       "│ (5) Getting data for histograms                │ 0.10s   │ 0.1%  │\n",
       "│ (6) Getting data for sequences                 │ 13.90s  │ 8.3%  │\n",
       "│ (7) Getting data for quantiles                 │ 0.08s   │ 0.0%  │\n",
       "└────────────────────────────────────────────────┴─────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTask                                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mTime   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mPct %\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━┩\n",
       "│ (1) Initialization                             │ 0.00s   │ 0.0%  │\n",
       "│ (2) Forward passes to gather model activations │ 118.18s │ 70.5% │\n",
       "│ (3) Computing feature acts from model acts     │ 35.48s  │ 21.2% │\n",
       "│ (4) Getting data for tables                    │ 0.01s   │ 0.0%  │\n",
       "│ (5) Getting data for histograms                │ 0.10s   │ 0.1%  │\n",
       "│ (6) Getting data for sequences                 │ 13.90s  │ 8.3%  │\n",
       "│ (7) Getting data for quantiles                 │ 0.08s   │ 0.0%  │\n",
       "└────────────────────────────────────────────────┴─────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46267e3ebf00461fbc93096cd1362421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving feature-centric vis:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "test_feature_idx_gpt = [i for i in range(50)]\n",
    "\n",
    "feature_vis_config_gpt = SaeVisConfig(\n",
    "    hook_point = sae.cfg.hook_name,\n",
    "    features = test_feature_idx_gpt,\n",
    "    batch_size = 4096,\n",
    "    verbose = True,\n",
    ")\n",
    "\n",
    "sae_vis_data_gpt = SaeVisData.create(\n",
    "    encoder = sae,\n",
    "    model = model,\n",
    "    tokens = all_tokens_gpt, # type: ignore\n",
    "    cfg = feature_vis_config_gpt,\n",
    ")\n",
    "\n",
    "filename = \"phys_features.html\"\n",
    "sae_vis_data_gpt.save_feature_centric_vis(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
